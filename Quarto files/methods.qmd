# Methods {#sec-methods style="text-align:justify;"}

::: callout-important
## Page under construction {style="text-align:justify;"}
:::

## Introduction

This chapter presents the methodology employed to carry out this study. The first section discusses statistical computing programming languages such as R. The second section introduces databases and their employment in archaeology, then outlines the construction of the database used for this research. GitHub as a hosting service for this research is discussed in section three. The fourth section describes the logic behind the choice of the temporal boundaries, the creation of chronologies and presents a possible solution to mitigate the problem of chronological fuzziness of samples.

## Data collection

-   Sources of data

-   Criteria and process of site selection

    -   Put periodization here?

    -   Introduce here the 'site types' problem?

    -   Put chronological fuzziness here?

-   General protocols followed

### Periodization {#sec-periodization style="text-align:justify;"}

Some of the statistics performed on the dataset have been based on sample periodization, from the Roman age to the Medieval age. The chronologies have been defined as follows:

-   **\[R\] Roman**: from the 1^st^ century BCE to the 2^nd^ century CE.

-   **\[LR\] Late Roman**: from the 3^rd^ to the 5^th^ century CE.

-   **\[EMA\] Early Middle Ages**: from the 6^th^ to the 10^th^ century CE.

-   **\[Ma\] Middle Ages:** from the 11^th^ century CE onwards.

In the database, the tables `faunal_chronologies` and `plants_chronologies` connect each bioarchaeological sample to another table with the identification numbers for the periods (e.g. Sample 1 = ID 1). If a sample has a chronology ranging between two periods, two separate entries will be recorded on the database (e.g. Sample 1 -- 2^nd^ to 3^rd^ c. CE = Periods: Roman, Late Roman) with the result of the sample being repeated in both periods.

![Periodization schema.](images/Periodization.png){style="text-align:center;" fig-align="center"}

### Chronological fuzziness {#sec-chrono-fuzziness style="text-align:justify;"}

::: callout-important
## Note {style="text-align:justify;"}

Remove?
:::

One of the methodological issues affecting this project is that of chronological fuzziness. Dating plant and animal remains using radiocarbon is very rare, at least in the samples recorded in the database. Most of these samples are dated using ceramics, and chronologies can range between one century or several. Taking this into account, I weighted each sample as follows: $$ W=\frac{1}{(C_{end}-C_{start}+1)} $$ Where:

-   $C\_{end}$ is the terminus ante quem.

-   $C\_{start}$ is the terminus post quem.

-   1 has been summed to the denominator to avoid 0 values.

The imported tables already contain a column of weights, as this operation has been performed on the database prior the export. A diachronic table of means for both datasets has been generated using the functions:

-   `zooarch_tables` (custom)
-   `Bot_mean_table` (custom)
-   `Bot_mean_fun` (custom)
-   `weightedMedian` (from the package `matrixStats`)

The weight can be used for weighted means and medians, with samples with larger chronologies (hence less precise/fuzzy) weighting less. This method provides each sample with a weight proportional to the length of its chronology so that lower weight values have a smaller impact on the computations.

### Site types

Write that for this project, decisions were made about how to classify archaeological sites in order to gather statistical data and store them in a database. Some sites, such as castra, are relatively easy to identify and categorize. However, there can be uncertainty when it comes to classifying sites as urban or rural, as these terms can have different meanings in the context of archaeology. In order to make these categorizations, it is important to consider the specific context and characteristics of each site, as well as the definitions and criteria being used to classify them.

https://www.sciencedirect.com/science/article/abs/pii/S2212054820300515

## Statistical analysis {style="text-align:justify;"}

-   Statistical approaches (statistical computing)

-   Software (R and packages)

-   What methods are applied to this thesis, why are there frequentist bits

-   Ecological analyses: ubiquity, richness, diversity

-   What is hypothesis testing? ANOVA and PERMANOVA

    -   Say why I used PERMANOVA

    -   What are the problems of HT? Get the reader ready for the Bayesian approach and try to find a way to justify the frequentist bits of this thesis.

-   Multivariate statistics

    -   General introduction

    -   Why they did not work in this thesis to explore the data?

    -   Dimensionality reduction: nMDS - it is non parametric. Say why I used it.

Statistical computing, often referred to as computational statistics, is a branch of statistics that uses computational approaches to solving statistical problems. Traditionally, the term *statistical computing* places more emphasis on numerical methods, while *computational statistics* refer to topics such as resampling methods, exploratory data analysis, neural networks, etc. [@rizzoStatisticalComputing2019]. Specifically, computational statistics deals with methods "unthinkable before the computer age" [@lauroComputationalStatisticsStatistical1996]. In archaeology, one widely used programming language for statistical computing is R, which has the advantage of relying on its strong academic community. The community actively contributes to the development of new packages, expanding the functionality and versatility of R for statistical computing. Researchers often choose R for statistical computing in archaeology due to its comprehensive statistical capabilities, flexibility, and the ability to reproduce and share analyses. The syntax and structure of R are specifically designed to facilitate data analysis and statistical modeling, allowing researchers to perform complex analyses efficiently. Additionally, R provides powerful data visualization tools, enabling researchers to create visually appealing and informative graphics to communicate their findings.

### The *R* programming language {style="text-align:justify;"}

R, as described on the [R-Project FAQs](https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-is-R_003f) is a "system for statistical computation and graphics. It consists of a language plus a run-time environment with graphics, a debugger, access to certain system functions, and the ability to run programs stored in script files". Increasingly popular for data scientists, R is based on S and provides its own IDE (the R GUI), although [RStudio](https://www.rstudio.com)[^methods-1] is the most popular IDE for computing the R language. For this project, RStudio was the standard IDE.

[^methods-1]: RStudio announced on July 2022 that its name will change to Posit, as it is expanding to Python and Visual Studio Code.

#### R Packages

In addition to base R, several packages enhance its performances and offer more tools to users. The packages are distributed by the official [CRAN repository](https://cran.r-project.org/web/packages/), which counts more than 19,533 packages[^methods-2].

[^methods-2]: Updated on May 19th, 2023.

##### tidyverse

The **tidyverse** ecosystem [@tidyverse] is a core set of packages for R, maintained by Hadley Wickham for importing, tidying, transforming and visualising data which includes packages such as---ggplot2, dplyr, tidyr, stringr, tibble, forcats, purr, readr.

##### ggplot2

**ggplot2** is the most common data visualisation package for R, included in the tidyverse environment. The package substitutes the R base graphics and allows visualisation of single and multiple components [@wickhamProgrammingGgplot22016].

##### knitr

The **knitr** engine enables the integration of R with HTML, Markdown and LaTeX. The package allows reproducible research [@knitr] and was used for generating dynamic reports and documentation for this thesis.

##### vegan

The **vegan** package [@oksanenVeganCommunityEcology2022] is designed for ordination methods, diversity analysis and multivariate analysis in ecology.

##### rethinking

The **rethinking** package [@rethinking] provides tools and resources for statistical modeling and data analysis in a Bayesian framework. Developed by Richard McElreath, it is specifically designed to support the concepts and techniques described in his book "Statistical Rethinking: A Bayesian Course with Examples in R and Stan" [@mcelreath2016].

### Archaeobotanical quantifications {style="text-align:justify;"}

::: callout-important
## Section in progress {style="text-align:justify;"}

**Issues**: problems with the data and general problems in sampling

**BIASES**: Talk about the biases and dataset problems here or do it in the Materials chapter?

Write that Quantifications on the archaeobotanical dataset were affected by several biases.
:::

#### Ubiquity {style="text-align:justify;"}

Ubiquity, or presence analysis, is a popular approach in archaeobotanical quantitative analysis. The method is straightforward---the number of sites/contexts where a plant is present is divided by the total number of sites/contexts under examination. If, for instance, an olive pit is present in 3 sites out of 10, the ubiquity for the olive will be 30%. The formula for the calculation is at follows:

$$
U_x = (\frac{N_p}{N_{tot}}) \cdot 100
$$ where $N_p$ is the number of presences, and $N_{tot}$ is the total number of contexts. The result can be multiplied by 100 to obtain a score in %.

This approach has both advantages and drawbacks. Presence analysis minimizes the impact of outliers (overrepresented plant species) on calculations [@wrightMethodologicalIssuesPaleoethnobotany2010, pp. 51-52], but the relative importance of a plant in a particular context is lost. It is also important to keep in mind that taxa richness is influenced by factors including sample size, deposition and preservation modes, and sampling strategies (e.g. sieving methodologies) [@pearsallPaleoethnobotanyHandbookProcedures2015a, pp. 161-2].

-   Remind the reader about the sample size problem. Somewhere (maybe Pearsall 2005 chapter ubiquity) there was a graph that showed how ubiquity is influenced by sample size

-   Write about the problems of this dataset that led to the choice of ubiquity as parameter

Ubiquity is the best option to immediately read the Italian peninsular botanical dataset. The variability in the seeds/fruits samples is too high, with different species being outliers in different sites. A likely reason for this is probably the poor sampling quality, usually occurring after an agglomerate of seeds is found during excavation. Normally, agglomerates are found in specific storage places or processing areas (e.g. wine/olive processing quarters), skewing the distribution of the curve. Ubiquity overcomes this issue, as it provides a score based on the percentage of presences of a plant species in the samples considered.

In addition to the general calculation of the diachronic ubiquity in the entire peninsula, it is also important to look for regional differences in the archaeobotanical dataset. To do so, I created an `R` function to subset data related to Northern, Central and Southern Italian regions. For a clearer reading of the plot, I divided the plants into--`Cereals`, `Pulses` and `Fruits/Nuts`. The results are in @sec-archaeobotany.

#### Diversity {style="text-align:justify;"}

::: {#species-richness-notes .callout-note}
## Notes:

Introduce the concept of diversity. Why is it useful?
:::

**Species richness** ($S$) is the number of species found within a community or ecosystem. The boundaries of the region are defined by the researcher. While ecologists use sampling or census to obtain the richness value, archaeobotanists can only rely on sampling, counting the presence of species in the area under investigation [@mooreDiversityTaxonomicFunctional2013]. **Species diversity** is a measurement of species richness combined with **species** **evenness**, meaning it takes into account not only how many species are present but also how evenly distributed the numbers of each species are.

There are different ways to calculate species diversity...

Shannon-W


## Multivariate statistics {#sec-met-multivariate-statistics style="text-align:justify;"}

::: callout-important
## Section in progress {style="text-align:justify;"}
:::

This research uses multivariate analysis to explore possible relationships within the sets of environmental data under investigation. Univariate analyses can be easily plotted and visualized with bar charts, histograms and density curves. A scatterplot (or scattergram) shows the relationship between two variables by plotting their values on the axes of a diagram using Cartesian coordinates. This relationship can also be mathematically measured by calculating a distance between two points in the graph. However, the relationship between more than two variables is much harder to read on a scatterplot, as it would require as many axes as the number of variables. If our analysis is exploratory, we might not know yet where to look for correlations. A possible solution is analysing each combination of two variables in the dataset and measure their correlation. This would require too much computation time if the dataset had a large number of variables, that is to say if we are dealing with big data. In addition, we would only gather information about the relationship between two variables, when there might be another factor influencing a trend or phenomenon. Multivariate statistics has a wide range of applications, including grouping and multidimensional scaling, as well as in machine learning and predictive analysis [@fletcherDiggingNumbersElementary2005]. In this research, multivariate methods have been applied both to the botanical and faunal datasets, mainly with the objective of *(i)* testing hypotheses between multiple variables and *(ii)* reducing the dimensionality of data to---assess which variables are the main drivers of change in the datasets and examine relationships between these variables.

### Statistical hypothesis testing {style="text-align:justify;"}

Explain what is hypothesis testing

#### PERMANOVA

**Permutational multivariate analysis of variance** (PERMANOVA) is a non-parametric multivariate statistical test used to compare group of objects. By using measure space, the null hypothesis that the centroids and dispersion of groups are identical is tested. The null hypothesis is rejected if either the centroid or the spread of the objects differs between the groups. A prior calculation of the distance between any two objects included in the experiment is used to determine whether the test is valid or not. [@andersonPermutationalMultivariateAnalysis2017].

### Dimensionality reduction and ordination {style="text-align:justify;"}

Dimensionality reduction techniques transform high-dimension datasets into a low-dimension ordination space, with the intention of maintaining the geometry and data structure as close as possible to the original dataset. The **dimension** of a dataset is given by the number of variables (*i.e.* the columns in the table). As anticipated in @sec-met-multivariate-statistics, as each variable is graphically represented by an axis, it would be virtually impossible to represent more than three axes in a graph. Ordination allows to reduce data dimensionality to usually one to three (at most) dimensions. Moreover, focusing on a reduced number of dimensions reduces the amount of "noise" that can mislead the interpretation [@gauchMultivariateAnalysisCommunity1982]. The points generated through ordination techniques (the objects in our dataset) can eventually be plotted in a scatterplot. In most of the ordination methods, points plotting closer together in graph are more similar, whereas points far apart from each other are more dissimilar [@shennanQuantifyingArchaeology1997, p. 197]. For instance, one could perform an ordination on a group of burials in a cemetery where each point represents a single burial assemblage. After the ordination it is also possible to group the new reduced set of variables, to observe differences between groups and facilitate the interpretation. In the previous example, a group might be represented by burials of the same ethnic group, status, etc.

Many of the ordination techniques described in this chapter developed in fields outside archaeology, and are thus borrowed from disciplines as community ecology. Ecologists regularly apply ordination methods for the study of environmental gradients, so that the term "gradient analysis" is often used interchangeably with "ordination". An environmental gradient refers to variations in site characteristics (*e.g.* time, elevation, temperature, vegetation, etc.), which in turn can affect biotic factors (*e.g.* species abundance, diversity, etc.) [@grebnerForestDynamics2013]. The purpose of ordination is then to identify the cause of ecological processes in the dataset. Generally, it is possible to apply ordination on datasets in which the variables have a cause-and-effect (*e.g.* climate vs. plant species) or mutual influences on each other. There are two main types of ordination, or gradient analysis, techniques (see @tbl-gradient-analysis): **direct** (constrained) or **indirect** (unconstrained). The objective of indirect (unconstrained) gradient analysis is to identify patterns between samples of 'dependent variables' (*e.g.* which sites are more similar according to their species composition). Conversely, direct gradient (or constrained) analysis includes more information (or tables) in a single analysis---the dependent variables are now constrained to be a function of other sets of 'independent variables' (usually environmental proxies). In short, a constrained analysis uses both datasets to find the best possible mathematical relationship between the dependent and independent variables. In this sense, direct gradient analysis can be considered as an extension of unconstrained analysis [@symsOrdination2008]. The choice between using constrained or unconstrained methods for ordination strictly depends on the research questions and on the researcher's dataset.

+-----------------+--------------------------------------------+-----------------------------------------+
| Response model  | Indirect gradient analysis                 | Direct gradient analysis                |
+=================+============================================+=========================================+
| ***Linear***    | Principal component analysis (PCA)         | Redundancy analysis (RDA)               |
+-----------------+--------------------------------------------+-----------------------------------------+
| ***Unimodal***  | Correspondence analysis (CA)               | Canonical correspondence analysis (CCA) |
|                 |                                            |                                         |
|                 | Detrended CA (DCA)                         | Detrended CCA                           |
+-----------------+--------------------------------------------+-----------------------------------------+
| ***Monotonic*** | non-metric multidimensional scaling (nMDS) |                                         |
+-----------------+--------------------------------------------+-----------------------------------------+

: Ordination methods used in gradient analysis. Table after @cuffneyAquaticEcosystemsIndicators2014 [p. 149] {#tbl-gradient-analysis}

#### PCA {style="text-align:justify;"}

::: callout-note
## Eigenvalues and eigenvectors {style="text-align:justify;"}

Both PCA and CA are methods based on eigenanalysis. The analysis of eigenvalues and eigenvectors will not be described in detail in this chapter, and the meaning of the terms will be only briefly explained in the text. However, some information must be given for the eigenanalysis-based ordination methods:

-   The methods are performed on a square symmetric matrix (for instance a correlation matrix);

-   The order of the variables is not important for the result;

-   Axes are ranked according to their eigenvalues (*i.e.* the first axis has the largest eigenvalue, the second the second-largest, etc.)

-   Eigenvalues have mathematical meaning that is important for the interpretation of our data.
:::

**Principal component analysis** (PCA) is an useful tool to identify possible patterns or sub-groups in data. The input variables in the dataset must be numerical (or at least dichotomous). If used for machine learning, PCA is in fact an *unsupervised* method, meaning that the data points will be unlabelled after the transformation. As for other dimensionality reduction methods, the algorithm tries to preserve the global structure of data by finding a space in which dimensions can be reduced without losing much information. In other words, PCA optimizes the loss of variance. The first step in PCA is creating a correlation matrix or covariance matrix, to extract the eigenvalues and eigenvectors. The decision between correlation or covariance matrix depends on our data and on our research questions. Correlation matrices are helpful if our variables have different units (*e.g.* lenght, volume, etc.). Covariance matrices can be used if the variables have the same unit and comparable variances. The latter is an important factor to consider before choosing covariance matrices, as the biggest variances will overshadow the others [@carlsonQuantitativeMethodsArchaeology2017, p. 267]. For PCA, the **eigenvalues** are the variances of the principal components (so that the first component has the largest eigenvalue) and the **eigenvectors** are the principal component loadings (the score for each variable in the new dimensional space). After extracting the eigenvalues and eigenvectors, the user must decide how many components he will use. The components (defined as the sum in % of the eigenvalues) are the axes in our new dimensional space. Most of the times, the first two components are enough to explain the variance in our dataset. For instance, if the first two axes explain 80% and 12% of the variance, it is acceptable to use a 2D graph---the graphical representation of the two axes will explain 92% of the variance. It is possible to choose the appropriate number of components by using a scree plot, which shows the amount of variance for each component. Scree plots are an important of PCA, because they allow us to see how many components have a significant variance (eigenvalue). A significant variance must be greater than 1. If more than 2 or 3 components have a variance greater than 1, PCA might not be the best method for reducing our set of data. The example in @fig-screeplot-pca-carlson2017 shows that PCA in this case is an appropriate method, as only the first two components have variances greater than 1. Therefore we can discard the other components as they are of little significance.

![Example of a scree plot showing that only the first two components have a variance greater than 1. Figure after @carlsonQuantitativeMethodsArchaeology2017 [p. 271]](images/Carlson2017p271-Scree%20plot%20PCA.png){#fig-screeplot-pca-carlson2017 fig-alt="Carlson 2017, p.271 - Scree plot" width="400"}

Most of the statistical packages offer an automatic rotation of the axes for an easier interpretation of the results, which is more important if the data has been standardised prior to the calculations.

##### Interpreting a PCA

PCA are often visualised through biplots, which shows at the same time the scores of the observations (rows) and the loadings of the variables (columns). Generally, the x-axis of the plot represents the first component, and the y-axis the second component. Making sense of the plotted **observations** is straightforward---points that are closer together are more similar and they can create clusters. It is important to remember that first component of the PCA lays on the x-axis, so differences in clusters on the first component (*i.e.* the horizontal distance) is actually larger than differences in clusters on the second component (*i.e.* the vertical distance). Interpreting the **variable** loadings can be more difficult. The variables are represented by arrows, implying that the variable increases in the direction of the arrow and decreases in the opposite direction. The direction of the arrow is given by the loadings of the first component---if the loadings are negative the arrows will point to the left. If the signs of the loadings are flipped the result will be the same but the arrows will point towards opposite directions. The arrows show how strongly each variable influences a principal component, with longer arrows being the best described in the dimension. The angle between the arrows also indicate extra information useful for the interpretation:

-   If two arrows form a small angle, the two variables are positively correlated.

-   If two arrows form a large diverging angle, the two variables are negatively correlated.

-   If two arrows form an angle close to 90°, there might be no correlation at all.

@fig-pca-carlson2017 shows an example of a PCA biplot. All the arrows (variables) point to the same direction, but we can interpret their correlations using the angles as described above. For example, B1 and B2 are not correlated, as they form an angle of 90°. On the contrary, the variables B and L are positively correlated, as they form a very small angle.

![PCA Biplot, after @carlsonQuantitativeMethodsArchaeology2017 [p.274]](images/Carlson2017p274-PCA.png){#fig-pca-carlson2017 fig-alt="PCA Biplot, Carlson 2017 p. 274" width="600"}

#### CA

**Correspondence analysis** (CA) is a dimensionality reduction technique that can be applied to contingency tables. Contingency tables (or crosstabs) are two-way tables used to display frequency data formed by two categorical variables. In some ways, correspondence analysis is similar to PCA, being an indirect gradient analysis[^methods-3] used to summarize categorical variables. As opposed to maximising the variance, CA maximises the degree of correspondence between rows and columns (*i.e.* observations and variables). The distance matrix used to perform the analysis is obtained from the chi-square distance between the rows and columns profiles. This is a modification of the chi-square statistic for hypothesis testing [@tallaridaChiSquareTest1987]. An in-depth technical and methodological breakdown of correspondence analysis is beyond the scope of this text, and readers can refer to @greenacreCorrespondenceAnalysisPractice2021. As in PCA, the analysis generates---eigenvalues, row coordinates (scores), and column coordinates (loadings). An important distinction must be made with PCA with regard to eigenvalues. In PCA, eigenvalues refer to the 'variation explained'. Conversely, in CA eigenvalues represent the **inertia**, the correlation coefficient between rows and columns (*i.e.* how well does the row correspond to the column?).

[^methods-3]: It is important not to confuse correspondence analysis (CA) with canonical correspondence analysis (CCA), which is a direct gradient analysis method.

Correspondence analysis is appropriate for exploring non-negative data (*e.g.* relative abundance, counts, presence/absence) in a contingency table, and examining the relationships between cells in a row, in a column, and their interrelationship [@baxterExploratoryMultivariateAnalysis2015, p. 101]. On a practical level, CA can be used to compare assemblages from different sites, to answer questions such as "which sites do certain finds correspond to?" or "which sites are more similar according to their assemblages?". This technique is extremely useful for archaeologists, even though its reception by the archaeological community has been slow. CA has been used by archaeologists since the mid-70s in France, although the technique was only introduced to the English-speaking audience in the 80s with a paper by @bolvikenCorrespondenceAnalysisAlternative1982, and became more popular in the 90s [@baxterCorrespondenceAnalysisArchaeologists2010; @baxterExploratoryMultivariateAnalysis2015]. One of the biggest drawbacks of CA is its sensitivity to outliers (values that are either much bigger or much smaller when compared to the others in the table). Some of the samples used in this project suffer from this problem, and the causes of outliers will be discussed in more detail later in this text[^methods-4]. The algorithm tends to place larger abundances towards the centre of the diagonal, and small or zero values away from the diagonal---it is indeed a bad idea to use CA with tables containing outliers. Outliers represent a major problem in CA, as their presence will determine the scale of the biplot, thus clustering the other points closer together, and perhaps obscuring some of the patterns we are interested in finding. In this eventuality, these points might be poorly ordinated. A possible way to overcome this issue would be to check whether the other dimensions are more appropriate for the representation, even though the sensible thing to do would be to remove these outliers from the original contingency table and to study them separately. Practical examples of outliers are rare species, for instance rows with a few non-zero values. Rare species should not be studied with CA, unless the table only contains rare species, with comparable values. As @baxterCorrespondenceAnalysisArchaeologists2010 [p. 225] note, we should not regard this omission as "subjective and unjustified data manipulation"---there are other ways to visualise outliers along with the rest of our data, for instance boxplots.

[^methods-4]: Insert here link to BOT bias and ZOO bias

In `R`, common implementations of correspondence analysis are included in the packages `MASS`, `FactoMineR` and `vegan`. CA has been used several times in this research using the function `cca()` from the `vegan` package, which was applied to the presence/absence archaeobotanical dataset (\*\*add where else\*\*[^methods-5]). The function `cca()` performs a canonical correspondence analysis (CCA), but it can also be used for correspondence analysis.

[^methods-5]: remember to change this

##### Interpreting a CA

Correspondence analyses are also visualised through biplots. As in PCA, the horizontal axis represents most of the variance, and the vertical axis is the second most representative dimension. A classic example for understanding and interpreting CA comes from a survey on the smoking habits of 193 employees in a fictitious company (@tbl-ca-smoking-example). The staff members are categorised according to their position in the company (rows) and smoking habits (columns).

| Staff                | None | Light | Medium | Heavy |
|----------------------|------|-------|--------|-------|
| Senior Manager (SM)  | 4    | 2     | 3      | 2     |
| Junior Manager (JM)  | 4    | 3     | 7      | 4     |
| Senior Employee (SE) | 25   | 10    | 12     | 4     |
| Junior Employee (JE) | 18   | 24    | 33     | 13    |
| Secretaries (SC)     | 10   | 6     | 7      | 2     |

: Smoking habits of staff groups, example after @greenacreTheoryApplicationsCorrespondence1984. {#tbl-ca-smoking-example}

The technical details of how the algorithm computes scores for this dataset are described in @greenacreCorrespondenceAnalysisPractice2021 [pp. 65-72]. After computing the CA, it is possible to visualise the biplot. In @fig-ca-example-smokers, the x-axis explains 87.8% of the variance, and the y-axis 11.8%. Together, they explain 99.6% of the total variance in the dataset, an optimal representation of the dataset. Much of the variance is explained by the first component (x-axis), and the horizontal distance between points is thus much more important than the vertical distance (on the y-axis).

![Correspondence analysis performed on @tbl-ca-smoking-example, image after @greenacreCorrespondenceAnalysisPractice2021 [p. 67].](images/Greenacre%202007%20figure%20p%2067.png){#fig-ca-example-smokers width="400"}

A scree plot can be helpful to evaluate the number of dimensions to choose, representing eigenvalues (or variances) from the largest to the smallest. The number of dimensions should be chosen by looking at where the scree plot starts to bend and the eigenvalues start to become small. An example of a scree plot has already been provided in @fig-screeplot-pca-carlson2017. The biplot contains points both for rows and columns. As the distances between points are chi-squared distances, for the interpretation of the results row points closer together are more similar, and the same logic applies to column points. However, one should be careful when interpreting distances between row and column points. The way to interpret their correlation is similar to the PCA:

-   If angle formed by the lines connecting the row and column labels to the origin is small, the association is strong.

-   If the angle is 90°, there is no association.

-   If the angle is near 180°, the association is negative.

In addition to what already mentioned, it is important to remember that points away from the origin and the edges are the most informative, as they might represent a better ordination.

Returning to the interpretation of the CA in @fig-ca-example-smokers, from the example concerning the smoking habits of the staff members in a company, it is easy to see how the smoking categories (on the left) are separated from the non-smokers (on the right). The groups that display the biggest separation are Junior Employee-Junior Manager, against Senior Employee. Secretaries are closer to Senior Employees. On the vertical axis, it is possible to see how Junior Employees are relatively more light smokers when compared to Junior Managers.

#### nMDS {#sec-met-nmds}

Multidimensional scaling (MDS) is a technique to visualise the level of similarity of individual observations (e.g. sites/cases) in a dataset. MDS works with matrices containing Euclidean distances between each pair of observations. Conversely, **non-metric multidimensional scaling** (nMDS) is a rank-based approach that finds both:

-   A non-parametric monotonic relationship between the items in the dissimilarity matrix and the Euclidean distances.

-   The location of items in the low-dimensional space.

The goal of nMDS is to represent the pairwise dissimilarity between items in the matrix as closely as possible. For this reason, it is considered as a good technique for multivariate data visualisation. nMDS can be used on quantitative, qualitative and mixed data. The `R` function [`metaMDS`](https://rdrr.io/rforge/vegan/man/metaMDS.html) from the package `vegan` allows users to select the distance metric most appropriate to their data (e.g. Bray-Curtis, Jaccard, etc.). As nMDS is an iterative approach, meaning that the the computations are run until the best solution is found, it can be quite computationally demanding for larger datasets. Although the nMDS algorithm tries to minimize the ordination stress, it is a good practice to compute the **ordination stress** value to judge the reliability of the solution found (goodness-of-fit). Ordination stress indicates how much distorted are the fitted data when compared to the original observed samples. Stress values can also be visualised with the function `stressplot()` (`vegan` package), which produces a Shepard stressplot (@fig-shepard-plot-nmds).

![A Shepard plot, from [R Studio Pubs](https://rpubs.com/CPEL/NMDS).](images/ShepardPlot.png){#fig-shepard-plot-nmds fig-align="center" width="600"}

The Shepard plot displays the ordination distance against the observed distance. Ideally, the higher the points should fall on a monotonic line, where an increased observed distance is related to an increased ordination distance. Moreover, the higher the number of dimensions, the lower the stress value. If interested in choosing the appropriate number of dimensions, it is possible to use a scree plot which shows the number of dimensions against the stress level. Generally, it is possible to interpret stress values following these guidelines [@dexterTroubleStressFlexible2018]:

| Interpretation                   | Stress level |
|----------------------------------|--------------|
| Excellent                        | \< 0.05      |
| Good                             | \< 0.1       |
| Usable (but caution is required) | \> 0.2       |
| Random                           | \> 0.3       |

: Ordination stress for the interpretation of nMDS {#tbl-stress-nmds}

If the solution has produced a good stress level for the number of dimensions required, it is possible to plot the nMDS and interpret the results. Points that plot closer together are more similar, while points that are distant one to each other are more different. The nMDS plot can also be useful in recognizing groups (points grouping together and plotting further from other points). An example is provided in @fig-nmds-rpubs.

![A nMDS plot with clusters, from [R Studio Pubs](https://rpubs.com/CPEL/NMDS).](images/NMDS_RPub.png){#fig-nmds-rpubs width="600"}

In this project, nMDS has been applied to the Roman and early Medieval cereal datasets, with the scope of dimensionality reduction. The dataset has been reduced to one dimension to show a greater degree of separation between Northern and Southern Italy in the early Middle Ages. The entire process of data preparation and processing is described in WRITE WHERE.

## Bayesian approach and multilevel modeling

-   Discuss the use of a Bayesian approach in the analysis.

-   Modeling counts

-   Intro to regression?

-   Explain the principles and advantages of multilevel modeling for the research design.

-   Appropriateness of the statistical method (Guisan et al, p. 55). Different types of data require different approaches and different probability distribution functions

-   Describe how the beta-binomial models were applied to account for overdispersion and class imbalance in the datasets.

-   Independence of observations: In the case of our type of data, this is not really possible so it is already violating this assumption

-   Why do we use credible intervals? http://doingbayesiandataanalysis.blogspot.com/2012/04/why-to-use-highest-density-intervals.html

### Betabinomial

::: callout-tip
Show the beta distribution and explain the precision values.
:::

Material: https://medium.com/@ro.mo.flo47/the-beta-binomial-model-an-introduction-to-bayesian-statistics-154395875f93

https://stats.stackexchange.com/questions/297901/choosing-between-uninformative-beta-priors/298176#298176

```{r}
#| echo: false
#| warning: false
#| message: false
library(rethinking)
pbar <- 0.5
theta <- 5
curve( dbeta2( x, pbar, theta), from=0, to=1, xlab="Probability", ylab="Density")
```

### Precision plots

The following plots for the posterior distributions of the beta-binomial models will also come with a precision parameter $\phi$, that shapes the beta curve. What is the correct way to interpret it? A $\phi$ value smaller than 2 indicates that there is extreme overdispersion in the dataset, and that probabilities around 0 and 1 are more likely than the mean. On the contrary, a $\phi$ value greater than 2 indicates more precision in the dataset. Although this is often the case of larger datasets, where most of the values shrink towards the average, it also indicates a coherent dataset (where values are similar). Finally, if $\phi$ equals 2 every probability between 0 and 1 is likely possible.

```{r}
#| echo: false
#| message: false
#| warning: false
#| layout-ncol: 3

library(rethinking)

# Create the plots 
pbar <- 0.5
theta <- 1
curve(dbeta2(x,pbar,theta), from=0, to=1,
      xlab="Probability", ylab="Density", main=expression(paste(phi<2)))

pbar <- 0.5
theta <- 2
curve(dbeta2(x,pbar,theta), from=0, to=1,
      xlab="Probability", ylab="Density", main=expression(paste(phi==2)))

pbar <- 0.5
theta <- 5
curve(dbeta2(x,pbar,theta), from=0, to=1,
      xlab="Probability", ylab="Density", main=expression(paste(phi>2)))


```

## Integration of Data Sources

-   Explain how the integration of different data sources was carried out.

-   Discuss the rationale behind integrating archaeological, environmental, and textual records for a comprehensive analysis.

-   Describe any challenges or considerations encountered during the integration process.


## Evaluation of Agricultural Strategies

-   Detail the methods used to evaluate agricultural strategies during the transition from the Roman Empire to the early medieval period in Italy.

-   Discuss the factors considered in assessing the role of political organization, economy, culture, and environment in shaping agricultural regimes and animal/plant husbandry selection.

## Limitations and Assumptions

-   Discuss any limitations or assumptions associated with the methods used.

-   Address potential biases or uncertainties in the data collection or analysis process.

-   Explain how these limitations were managed or considered in the interpretation of the results.

## GitHub: hosting the project

::: callout-important
## Section under construction {style="text-align:justify;"}

This section will include a general intro to GitHub and why I chose to host data there.
:::

The code and the datasets used to build this research will be hosted publicly on GitHub to allow reproducibility.
